{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DX799S O1 Data Science Capstone (Summer 1 2025): ACTIVITY 5.2 ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each week, you will apply the concepts of that week to your Integrated Capstone Projectâ€™s dataset. In preparation for Milestone One, create a Jupyter Notebook (similar to in Module B, Semester Two) that illustrates these lessons. There are no specific questions to answer in your Jupyter Notebook files in this course; your general goal is to analyze your data using the methods you have learned about in this course and in this program and draw interesting conclusions. \n",
    "\n",
    "For Week 5, include concepts such as support vector machines, the kernel trick, and regularization for support vector machines. Complete your Jupyter Notebook homework by 11:59pm ET on Sunday. \n",
    "\n",
    "In Week 7, you will compile your findings from your Jupyter Notebook homework into your Milestone One assignment for grading. For full instructions and the rubric for Milestone One, refer to the following link. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following dataset, \"Video Review\", is a collection of information that was created based on reviewable video evidence that outlines the events that resulted in a concussion during punt players in the NFL 2016-2017 season. The target, Primary_Impact_Type, outlines if the concussion occurred from the impact of Helmet-to-Helmet, Helmet-to-Body, or Helmet-to-Ground."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy Video Review Dataset: 0.7586206896551724\n",
      "\n",
      "Classification Report (Video Review Dataset):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.86      0.77        14\n",
      "           1       1.00      1.00      1.00         1\n",
      "           2       0.80      0.62      0.70        13\n",
      "           3       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.76        29\n",
      "   macro avg       0.88      0.87      0.87        29\n",
      "weighted avg       0.77      0.76      0.75        29\n",
      "\n",
      "\n",
      "Confusion Matrix (Video Review Dataset):\n",
      " [[12  0  2  0]\n",
      " [ 0  1  0  0]\n",
      " [ 5  0  8  0]\n",
      " [ 0  0  0  1]]\n"
     ]
    }
   ],
   "source": [
    "#Video Review Dataset with support vector machines\n",
    "\n",
    "df_videoreview = pd.read_csv(\"video_review.csv\")\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for col in df_videoreview.select_dtypes(include=['object']).columns:\n",
    "    df_videoreview[col] = label_encoder.fit_transform(df_videoreview[col].astype(str))\n",
    "\n",
    "\n",
    "target_column = 'Primary_Impact_Type'  \n",
    "X = df_videoreview.drop(columns=[target_column])\n",
    "y = df_videoreview[target_column]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "svm = SVC(kernel='rbf', decision_function_shape='ovr', class_weight='balanced', random_state=42)\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_train = svm.predict(X_train_scaled)\n",
    "\n",
    "print(\"Training Accuracy Video Review Dataset:\", accuracy_score(y_train, y_pred_train))\n",
    "print(\"\\nClassification Report (Video Review Dataset):\\n\", classification_report(y_train, y_pred_train))\n",
    "print(\"\\nConfusion Matrix (Video Review Dataset):\\n\", confusion_matrix(y_train, y_pred_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C Video Review Dataset: 1\n",
      "Best cross-val accuracy Video Review Dataset: 0.4133333333333333\n",
      "Training set accuracy with best C Video Review Dataset: 0.7586206896551724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Video Review Dataset regularization for support vector machines\n",
    "\n",
    "param_grid = {'C': [0.01, 0.1, 1, 10, 100]}\n",
    "grid = GridSearchCV(SVC(kernel='rbf', class_weight='balanced', random_state=42), param_grid, cv=5)\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Best C Video Review Dataset:\", grid.best_params_['C'])\n",
    "print(\"Best cross-val accuracy Video Review Dataset:\", grid.best_score_)\n",
    "\n",
    "train_accuracy = grid.best_estimator_.score(X_train_scaled, y_train)\n",
    "print(\"Training set accuracy with best C Video Review Dataset:\", train_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Video Review Dataset Training SVM with linear kernel:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.64      0.64        14\n",
      "           1       1.00      1.00      1.00         1\n",
      "           2       0.62      0.62      0.62        13\n",
      "           3       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.66        29\n",
      "   macro avg       0.81      0.81      0.81        29\n",
      "weighted avg       0.66      0.66      0.66        29\n",
      "\n",
      "\n",
      " Video Review Dataset Training SVM with poly kernel:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.64      0.78        14\n",
      "           1       1.00      1.00      1.00         1\n",
      "           2       0.72      1.00      0.84        13\n",
      "           3       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.83        29\n",
      "   macro avg       0.93      0.91      0.91        29\n",
      "weighted avg       0.88      0.83      0.82        29\n",
      "\n",
      "\n",
      " Video Review Dataset Training SVM with rbf kernel:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.86      0.77        14\n",
      "           1       1.00      1.00      1.00         1\n",
      "           2       0.80      0.62      0.70        13\n",
      "           3       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.76        29\n",
      "   macro avg       0.88      0.87      0.87        29\n",
      "weighted avg       0.77      0.76      0.75        29\n",
      "\n",
      "\n",
      " Video Review Dataset Training SVM with sigmoid kernel:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.43      0.50        14\n",
      "           1       0.14      1.00      0.25         1\n",
      "           2       0.60      0.46      0.52        13\n",
      "           3       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.48        29\n",
      "   macro avg       0.46      0.72      0.48        29\n",
      "weighted avg       0.58      0.48      0.51        29\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Video Review Dataset the kernel trick \n",
    "\n",
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "\n",
    "for kernel in kernels:\n",
    "    print(f\"\\n Video Review Dataset Training SVM with {kernel} kernel:\")\n",
    "    svm = SVC(kernel=kernel, degree=3 if kernel == 'poly' else 3, class_weight='balanced', random_state=42)\n",
    "    svm.fit(X_train_scaled, y_train)\n",
    "    y_pred_train = svm.predict(X_train_scaled)\n",
    "    print(classification_report(y_train, y_pred_train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next dataset, Injury Record, looks to determine the relationship between the playing surface and the injury and performance of NFL athletes. The Injury Record dataset accounts for 105 lower-limbs injuries that occurred over two seasons during the regular NFL season and provides information on the surface the game occurred on and the number of days the player missed due to injury (or how severe it was). The target in this case is surface which lists the type of surface (synethic or natural) the field was when the injury occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_injuryrecord = pd.read_csv(\"InjuryRecord.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy Injury Record Dataset: 0.6547619047619048\n",
      "\n",
      "Classification Report (Injury Record Dataset):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.64      0.63        39\n",
      "           1       0.68      0.67      0.67        45\n",
      "\n",
      "    accuracy                           0.65        84\n",
      "   macro avg       0.65      0.65      0.65        84\n",
      "weighted avg       0.66      0.65      0.66        84\n",
      "\n",
      "\n",
      "Confusion Matrix (Injury Record Dataset):\n",
      " [[25 14]\n",
      " [15 30]]\n"
     ]
    }
   ],
   "source": [
    "#Injury Record Dataset with support vector machines\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for col in df_injuryrecord.select_dtypes(include=['object']).columns:\n",
    "    df_injuryrecord[col] = label_encoder.fit_transform(df_injuryrecord[col].astype(str))\n",
    "\n",
    "\n",
    "target_column = 'Surface'  \n",
    "X_injury = df_injuryrecord.drop(columns=[target_column])\n",
    "y_injury = df_injuryrecord[target_column]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_injury, y_injury, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "\n",
    "svm_injury = SVC(kernel='rbf', decision_function_shape='ovr', class_weight='balanced', random_state=42)\n",
    "svm_injury.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_train = svm_injury.predict(X_train_scaled)\n",
    "\n",
    "print(\"Training Accuracy Injury Record Dataset:\", accuracy_score(y_train, y_pred_train))\n",
    "print(\"\\nClassification Report (Injury Record Dataset):\\n\", classification_report(y_train, y_pred_train))\n",
    "print(\"\\nConfusion Matrix (Injury Record Dataset):\\n\", confusion_matrix(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C Injury Record: 0.01\n",
      "Best cross-val accuracy Injury Record Dataset: 0.5110294117647058\n",
      "Training set accuracy with best C Injury Record Dataset: 0.5357142857142857\n"
     ]
    }
   ],
   "source": [
    "#Injury Record Dataset regularization for support vector machines\n",
    "param_grid_injury = {'C': [0.01, 0.1, 1, 10, 100]}\n",
    "grid_injury = GridSearchCV(SVC(kernel='rbf', class_weight='balanced', random_state=42), param_grid_injury, cv=5)\n",
    "grid_injury.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Best C Injury Record:\", grid_injury.best_params_['C'])\n",
    "print(\"Best cross-val accuracy Injury Record Dataset:\", grid_injury.best_score_)\n",
    "\n",
    "train_accuracy_injury = grid_injury.best_estimator_.score(X_train_scaled, y_train)\n",
    "print(\"Training set accuracy with best C Injury Record Dataset:\", train_accuracy_injury)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Injury Record Dataset Training SVM with linear kernel:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.46      0.49        39\n",
      "           1       0.58      0.64      0.61        45\n",
      "\n",
      "    accuracy                           0.56        84\n",
      "   macro avg       0.55      0.55      0.55        84\n",
      "weighted avg       0.56      0.56      0.56        84\n",
      "\n",
      "\n",
      " Injury Record Dataset Training SVM with poly kernel:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.44      0.53        39\n",
      "           1       0.63      0.82      0.71        45\n",
      "\n",
      "    accuracy                           0.64        84\n",
      "   macro avg       0.65      0.63      0.62        84\n",
      "weighted avg       0.65      0.64      0.63        84\n",
      "\n",
      "\n",
      " Injury Record Dataset Training SVM with rbf kernel:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.64      0.63        39\n",
      "           1       0.68      0.67      0.67        45\n",
      "\n",
      "    accuracy                           0.65        84\n",
      "   macro avg       0.65      0.65      0.65        84\n",
      "weighted avg       0.66      0.65      0.66        84\n",
      "\n",
      "\n",
      " Injury Record Dataset Training SVM with sigmoid kernel:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.51      0.51        39\n",
      "           1       0.57      0.56      0.56        45\n",
      "\n",
      "    accuracy                           0.54        84\n",
      "   macro avg       0.53      0.53      0.53        84\n",
      "weighted avg       0.54      0.54      0.54        84\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Injury Record Dataset the kernel trick \n",
    "\n",
    "kernels_injury = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "\n",
    "for kernel in kernels_injury:\n",
    "    print(f\"\\n Injury Record Dataset Training SVM with {kernel} kernel:\")\n",
    "    svm_injury = SVC(kernel=kernel, degree=3 if kernel == 'poly' else 3, class_weight='balanced', random_state=42)\n",
    "    svm_injury.fit(X_train_scaled, y_train)\n",
    "    y_pred_train = svm_injury.predict(X_train_scaled)\n",
    "    print(classification_report(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last dataset, PlayList, \"contains information about each player-play in the dataset, to include the playerâ€™s assigned roster position, stadium type, field type, weather, play type, position for the play, and position group\". This dataset was provided with the injury_record dataset so it provides additional information regarding the environment when a player's lower body injury occurred during these two NFL seasons. The target in this case will be \"PlayType\" which can include kickoff, run, pass, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy Playlist Dataset: 0.14784835489972098\n",
      "\n",
      "Classification Report (PlaylistDataset):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.01      0.90      0.01       215\n",
      "           1       0.05      0.22      0.09      4762\n",
      "           2       0.04      0.22      0.07      3948\n",
      "           3       0.14      0.51      0.22      4615\n",
      "           4       0.12      0.34      0.18      3741\n",
      "           5       0.09      0.42      0.15      2225\n",
      "           6       0.64      0.14      0.23    110422\n",
      "           7       0.11      0.52      0.19      4566\n",
      "           8       0.07      0.28      0.11      2751\n",
      "           9       0.09      0.38      0.15      1991\n",
      "          10       0.45      0.07      0.12     74062\n",
      "          11       0.01      0.86      0.02       306\n",
      "\n",
      "    accuracy                           0.15    213604\n",
      "   macro avg       0.15      0.41      0.13    213604\n",
      "weighted avg       0.50      0.15      0.18    213604\n",
      "\n",
      "\n",
      "Confusion Matrix (Playlist Dataset):\n",
      " [[  194     1     4     8     0     0     0     8     0     0     0     0]\n",
      " [  552  1065   604   374   273   288   116   428   273   218    54   517]\n",
      " [  572   483   877   332   179   198   103   384   222   168    41   389]\n",
      " [  402    56    88  2376     0     0    75  1592     2     0    17     7]\n",
      " [    4    35    36    21  1277   853    29    25   555   709     4   193]\n",
      " [    0    15    17    11   394   940    21     7   309   388     5   118]\n",
      " [16271 10674 11211  7130  4721  4509 15702  9344  5288  3252  6010 16310]\n",
      " [  448    42    83  1531     0     0    48  2382     1     5    26     0]\n",
      " [    3    36    34     3   429   585    30    15   764   629     9   214]\n",
      " [    0    11    18     8   287   428    11     7   366   762     4    89]\n",
      " [10906  7798  7763  5049  2841  2889  8388  6837  3506  2184  4980 10921]\n",
      " [    2     2     3     1     8    11     0     0    10     7     0   262]]\n"
     ]
    }
   ],
   "source": [
    "#Playlist Dataset with support vector machines\n",
    "\n",
    "df_playlist = pd.read_csv(\"PlayList.csv\")\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "\n",
    "for col in df_playlist.select_dtypes(include=['object']).columns:\n",
    "    df_playlist[col] = label_encoder.fit_transform(df_playlist[col].astype(str))\n",
    "\n",
    "\n",
    "target_column = 'PlayType'  \n",
    "X_playlist = df_playlist.drop(columns=[target_column])\n",
    "y_playlist = df_playlist[target_column]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_playlist, y_playlist, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "svm_playlist = SVC(kernel='rbf', decision_function_shape='ovr', class_weight='balanced', random_state=42)\n",
    "svm_playlist.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_train = svm_playlist.predict(X_train_scaled)\n",
    "\n",
    "print(\"Training Accuracy Playlist Dataset:\", accuracy_score(y_train, y_pred_train))\n",
    "print(\"\\nClassification Report (PlaylistDataset):\\n\", classification_report(y_train, y_pred_train))\n",
    "print(\"\\nConfusion Matrix (Playlist Dataset):\\n\", confusion_matrix(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_playlist = pd.read_csv(\"PlayList.csv\")\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "\n",
    "for col in df_playlist.select_dtypes(include=['object']).columns:\n",
    "    df_playlist[col] = label_encoder.fit_transform(df_playlist[col].astype(str))\n",
    "\n",
    "\n",
    "target_column = 'PlayType'  \n",
    "X_playlist = df_playlist.drop(columns=[target_column])\n",
    "y_playlist = df_playlist[target_column]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_playlist, y_playlist, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The following code blocks ran for over 5 hours then timed out due to the dataset size. In the future, going back and applying EDA to this new dataset, would most likly help the code run more efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Playlist Dataset with regularization for support vector machines\n",
    "param_grid_playlist = {'C': [0.01, 0.1, 1, 10, 100]}\n",
    "grid_playlist= GridSearchCV(SVC(kernel='rbf', class_weight='balanced', random_state=42), param_grid_playlist, cv=5)\n",
    "grid_playlist.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Best C Playlist:\", grid_playlist.best_params_['C'])\n",
    "print(\"Best cross-val accuracy Playlist Dataset:\", grid_playlist.best_score_)\n",
    "\n",
    "train_accuracy_injury = grid_playlist.best_estimator_.score(X_train_scaled, y_train)\n",
    "print(\"Training set accuracy with best C Playlist Dataset:\", train_accuracy_injury)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Playlist Dataset with the kernel trick \n",
    "\n",
    "kernels_playlist = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "\n",
    "for kernel in kernels_playlist:\n",
    "    print(f\"\\n Injury Record Dataset Training SVM with {kernel} kernel:\")\n",
    "    svm_playlist = SVC(kernel=kernel, degree=3 if kernel == 'poly' else 3, class_weight='balanced', random_state=42)\n",
    "    svm_playlist.fit(X_train_scaled, y_train)\n",
    "    y_pred_train = svm_playlist.predict(X_train_scaled)\n",
    "    print(classification_report(y_train, y_pred_train))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
