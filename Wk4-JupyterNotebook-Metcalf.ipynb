{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DX799S O1 Data Science Capstone (Summer 1 2025): ACTIVITY 4.2 ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each week, you will apply the concepts of that week to your Integrated Capstone Project’s dataset. In preparation for Milestone One, create a Jupyter Notebook (similar to in Module B, semester two) that illustrates these lessons. There are no specific questions to answer in your Jupyter Notebook files in this course; your general goal is to analyze your data using the methods you have learned about in this course and in this program and draw interesting conclusions. \n",
    "\n",
    "For Week 4, include concepts such as logistic regression and feature scaling. This homework should be submitted for peer review in the assignment titled 4.3 Peer Review: Week 4 Jupyter Notebook. Complete and submit your Jupyter Notebook homework by 11:59pm ET on Sunday. \n",
    "\n",
    "In Week 7, you will compile your findings from your Jupyter Notebook homework into your Milestone One assignment for grading. For full instructions and the rubric for Milestone One, refer to the following link.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# Useful Imports: Add more as needed\n",
    "# ===================================\n",
    "\n",
    "# Standard Libraries\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from itertools import chain, combinations\n",
    "\n",
    "# Data Science Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.ticker as mticker  # Optional: Format y-axis labels as dollars\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn (Machine Learning)\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, \n",
    "    cross_val_score, \n",
    "    GridSearchCV, \n",
    "    RandomizedSearchCV, \n",
    "    RepeatedKFold\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import SequentialFeatureSelector, f_regression, SelectKBest\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following dataset, \"Video Review\", is a collection of information that was created based on reviewable video evidence that outlines the events that resulted in a concussion during punt players in the NFL 2016-2017 season. The target, Primary_Impact_Type, outlines if the concussion occurred from the impact of Helmet-to-Helmet, Helmet-to-Body, or Helmet-to-Ground."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object columns before encoding:\n",
      "Index(['Player_Activity_Derived', 'Turnover_Related', 'Primary_Impact_Type',\n",
      "       'Primary_Partner_GSISID', 'Primary_Partner_Activity_Derived',\n",
      "       'Friendly_Fire'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#Video Review Dataset with Feature Scaling\n",
    "\n",
    "df_videoreview = pd.read_csv(\"video_review.csv\")\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "print(\"Object columns before encoding:\")\n",
    "print(df_videoreview.select_dtypes(include=['object']).columns)\n",
    "\n",
    "for col in df_videoreview.select_dtypes(include=['object']).columns:\n",
    "    df_videoreview[col] = label_encoder.fit_transform(df_videoreview[col].astype(str))\n",
    "\n",
    "\n",
    "target_column = 'Primary_Impact_Type'  \n",
    "X = df_videoreview.drop(columns=[target_column])\n",
    "y = df_videoreview[target_column]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Video Review Dataset: 0.6897\n",
      "Precision for Video Review Dataset: 0.6897\n",
      "F1-score for Video Review Dataset: 0.6888\n"
     ]
    }
   ],
   "source": [
    "#Video Review Dataset Logistic Regression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_train_pred = model.predict(X_train_scaled)\n",
    "\n",
    "accuracy = accuracy_score(y_train, y_train_pred)\n",
    "precision = precision_score(y_train, y_train_pred, average=\"weighted\") \n",
    "f1 = f1_score(y_train, y_train_pred, average=\"weighted\")\n",
    "\n",
    "print(f\"Accuracy for Video Review Dataset: {accuracy:.4f}\")\n",
    "print(f\"Precision for Video Review Dataset: {precision:.4f}\")\n",
    "print(f\"F1-score for Video Review Dataset: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next dataset, Injury Record, looks to determine the relationship between the playing surface and the injury and performance of NFL athletes. The Injury Record dataset accounts for 105 lower-limbs injuries that occurred over two seasons during the regular NFL season and provides information on the surface the game occurred on and the number of days the player missed due to injury (or how severe it was). The target in this case is surface which lists the type of surface (synethic or natural) the field was when the injury occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_injuryrecord = pd.read_csv(\"InjuryRecord.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object columns before encoding:\n",
      "Index(['GameID', 'PlayKey', 'BodyPart', 'Surface'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#Injury Record Dataset with Feature Scaling\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "print(\"Object columns before encoding:\")\n",
    "print(df_injuryrecord.select_dtypes(include=['object']).columns)\n",
    "\n",
    "for col in df_injuryrecord.select_dtypes(include=['object']).columns:\n",
    "    df_injuryrecord[col] = label_encoder.fit_transform(df_injuryrecord[col].astype(str))\n",
    "\n",
    "\n",
    "target_column = 'Surface'  \n",
    "X_injury = df_injuryrecord.drop(columns=[target_column])\n",
    "y_injury = df_injuryrecord[target_column]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_injury, y_injury, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Injury Record Dataset: 0.5595\n",
      "Precision for Injury Record Dataset: 0.5548\n",
      "F1-score for Injury Record Dataset: 0.5474\n"
     ]
    }
   ],
   "source": [
    "#Injury Record Dataset Logistic Regression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_train_pred = model.predict(X_train_scaled)\n",
    "\n",
    "accuracy = accuracy_score(y_train, y_train_pred)\n",
    "precision = precision_score(y_train, y_train_pred, average=\"weighted\") \n",
    "f1 = f1_score(y_train, y_train_pred, average=\"weighted\")\n",
    "\n",
    "print(f\"Accuracy for Injury Record Dataset: {accuracy:.4f}\")\n",
    "print(f\"Precision for Injury Record Dataset: {precision:.4f}\")\n",
    "print(f\"F1-score for Injury Record Dataset: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last dataset, PlayList, \"contains information about each player-play in the dataset, to include the player’s assigned roster position, stadium type, field type, weather, play type, position for the play, and position group\". This dataset was provided with the injury_record dataset so it provides additional information regarding the environment when a player's lower body injury occurred during these two NFL seasons. The target in this case will be \"PlayType\" which can include kickoff, run, pass, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object columns before encoding:\n",
      "Index(['GameID', 'PlayKey', 'RosterPosition', 'StadiumType', 'FieldType',\n",
      "       'Weather', 'PlayType', 'Position', 'PositionGroup'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#Playlist Dataset with Feature Scaling\n",
    "\n",
    "df_playlist = pd.read_csv(\"PlayList.csv\")\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "print(\"Object columns before encoding:\")\n",
    "print(df_playlist.select_dtypes(include=['object']).columns)\n",
    "\n",
    "for col in df_playlist.select_dtypes(include=['object']).columns:\n",
    "    df_playlist[col] = label_encoder.fit_transform(df_playlist[col].astype(str))\n",
    "\n",
    "\n",
    "target_column = 'PlayType'  \n",
    "X_playlist = df_playlist.drop(columns=[target_column])\n",
    "y_playlist = df_playlist[target_column]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_playlist, y_playlist, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Playlist Dataset: 0.5167\n",
      "Precision for Playlist Dataset: 0.2769\n",
      "F1-score for Playlist Dataset: 0.3546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "#Playlist Dataset with Logistic Regression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_train_pred = model.predict(X_train_scaled)\n",
    "\n",
    "accuracy = accuracy_score(y_train, y_train_pred)\n",
    "precision = precision_score(y_train, y_train_pred, average=\"weighted\") \n",
    "f1 = f1_score(y_train, y_train_pred, average=\"weighted\")\n",
    "\n",
    "print(f\"Accuracy for Playlist Dataset: {accuracy:.4f}\")\n",
    "print(f\"Precision for Playlist Dataset: {precision:.4f}\")\n",
    "print(f\"F1-score for Playlist Dataset: {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
