{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DX799S O1 Data Science Capstone (Summer 1 2025): ACTIVITY 4.2 ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each week, you will apply the concepts of that week to your Integrated Capstone Projectâ€™s dataset. In preparation for Milestone One, create a Jupyter Notebook (similar to in Module B, semester two) that illustrates these lessons. There are no specific questions to answer in your Jupyter Notebook files in this course; your general goal is to analyze your data using the methods you have learned about in this course and in this program and draw interesting conclusions. \n",
    "\n",
    "For Week 4, include concepts such as logistic regression and feature scaling. This homework should be submitted for peer review in the assignment titled 4.3 Peer Review: Week 4 Jupyter Notebook. Complete and submit your Jupyter Notebook homework by 11:59pm ET on Sunday. \n",
    "\n",
    "In Week 7, you will compile your findings from your Jupyter Notebook homework into your Milestone One assignment for grading. For full instructions and the rubric for Milestone One, refer to the following link.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import cross_val_predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following dataset, \"Video Review\", is a collection of information that was created based on reviewable video evidence that outlines the events that resulted in a concussion during punt players in the NFL 2016-2017 season. The target, Primary_Impact_Type, outlines if the concussion occurred from the impact of Helmet-to-Helmet, Helmet-to-Body, or Helmet-to-Ground."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object columns before encoding:\n",
      "Index(['Player_Activity_Derived', 'Turnover_Related', 'Primary_Impact_Type',\n",
      "       'Primary_Partner_GSISID', 'Primary_Partner_Activity_Derived',\n",
      "       'Friendly_Fire'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#Video Review Dataset with Feature Scaling\n",
    "\n",
    "df_videoreview = pd.read_csv(\"video_review.csv\")\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "print(\"Object columns before encoding:\")\n",
    "print(df_videoreview.select_dtypes(include=['object']).columns)\n",
    "\n",
    "for col in df_videoreview.select_dtypes(include=['object']).columns:\n",
    "    df_videoreview[col] = label_encoder.fit_transform(df_videoreview[col].astype(str))\n",
    "\n",
    "\n",
    "target_column = 'Primary_Impact_Type'  \n",
    "X = df_videoreview.drop(columns=[target_column])\n",
    "y = df_videoreview[target_column]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C value: 100\n",
      "Cross-validated Accuracy for Video Review Dataset: 0.5946\n",
      "Cross-validated Precision for Video Review Dataset: 0.5903\n",
      "Cross-validated F1-score for Video Review Datasett: 0.5871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "#Video Review Dataset Logistic Regression\n",
    "\n",
    "\n",
    "pipeline_video = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'model__C': [0.01, 0.1, 1, 10, 100, 200, 300]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline_video,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1_weighted',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred = cross_val_predict(best_model, X, y, cv=5)\n",
    "\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "precision = precision_score(y, y_pred, average=\"weighted\") \n",
    "f1 = f1_score(y, y_pred, average=\"weighted\")\n",
    "\n",
    "print(f\"Best C value: {grid_search.best_params_['model__C']}\")\n",
    "print(f\"Cross-validated Accuracy for Video Review Dataset: {accuracy:.4f}\")\n",
    "print(f\"Cross-validated Precision for Video Review Dataset: {precision:.4f}\")\n",
    "print(f\"Cross-validated F1-score for Video Review Datasett: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next dataset, Injury Record, looks to determine the relationship between the playing surface and the injury and performance of NFL athletes. The Injury Record dataset accounts for 105 lower-limbs injuries that occurred over two seasons during the regular NFL season and provides information on the surface the game occurred on and the number of days the player missed due to injury (or how severe it was). The target in this case is surface which lists the type of surface (synethic or natural) the field was when the injury occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_injuryrecord = pd.read_csv(\"InjuryRecord.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object columns before encoding:\n",
      "Index(['GameID', 'PlayKey', 'BodyPart', 'Surface'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#Injury Record Dataset with Feature Scaling\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "print(\"Object columns before encoding:\")\n",
    "print(df_injuryrecord.select_dtypes(include=['object']).columns)\n",
    "\n",
    "for col in df_injuryrecord.select_dtypes(include=['object']).columns:\n",
    "    df_injuryrecord[col] = label_encoder.fit_transform(df_injuryrecord[col].astype(str))\n",
    "\n",
    "\n",
    "target_column = 'Surface'  \n",
    "X_injury = df_injuryrecord.drop(columns=[target_column])\n",
    "y_injury = df_injuryrecord[target_column]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_injury, y_injury, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C value: 0.1\n",
      "Cross-validated Accuracy for Injury Record Dataset: 0.5048\n",
      "Cross-validated Precision for Injury Record Dataset: 0.4900\n",
      "Cross-validated F1-score for Injury Record Dataset: 0.4857\n"
     ]
    }
   ],
   "source": [
    "#Injury Record Dataset Logistic Regression\n",
    "\n",
    "pipeline_injury = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'model__C': [0.01, 0.1, 1, 10, 100, 200, 300]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline_injury,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1_weighted',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_injury, y_injury)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred_injury = cross_val_predict(best_model, X_injury, y_injury, cv=5)\n",
    "\n",
    "accuracy_injury = accuracy_score(y_injury, y_pred_injury)\n",
    "precision_injury = precision_score(y_injury, y_pred_injury, average=\"weighted\") \n",
    "f1_injury = f1_score(y_injury, y_pred_injury, average=\"weighted\")\n",
    "\n",
    "print(f\"Best C value: {grid_search.best_params_['model__C']}\")\n",
    "print(f\"Cross-validated Accuracy for Injury Record Dataset: {accuracy_injury:.4f}\")\n",
    "print(f\"Cross-validated Precision for Injury Record Dataset: {precision_injury:.4f}\")\n",
    "print(f\"Cross-validated F1-score for Injury Record Dataset: {f1_injury:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last dataset, Concussion, contains a list of concussion injuries that occurred in the National Football League from the year 2012 to 2014. The data includes features such as Position, Pre-Season Injury?, Week of Injury, Weeks Injured, Games Missed, Reported Injury Type, Average Playtime Before Injury, etc. The target in this case will be \"Reported Injury Type\" which will be limited to just concussions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concussion Dataset with Feature Scaling\n",
    "\n",
    "df_concussion = pd.read_csv(\"Concussion Injuries 2012-2014 (1).csv\")\n",
    "df_clean_concussion = df_concussion.drop(columns=['ID', 'Player', 'Game', 'Date', 'Winning Team?', 'Unknown Injury?'])\n",
    "df_clean_concussion = df_clean_concussion.dropna()\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for col in df_clean_concussion.select_dtypes(include=['object']).columns:\n",
    "    df_clean_concussion[col] = label_encoder.fit_transform(df_clean_concussion[col].astype(str))\n",
    "\n",
    "target_column = 'Reported Injury Type'  \n",
    "X_concussion = df_clean_concussion.drop(columns=[target_column])\n",
    "y_concussion = df_clean_concussion[target_column]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_concussion, y_concussion, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C value: 100\n",
      "Cross-validated Accuracy for Concussion Dataset: 0.8031\n",
      "Cross-validated Precision for Concussion Dataset: 0.7681\n",
      "Cross-validated F1-score for Concussion Dataset: 0.7781\n"
     ]
    }
   ],
   "source": [
    "#Concussion Dataset with Logistic Regression\n",
    "\n",
    "pipeline_concussion = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'model__C': [0.01, 0.1, 1, 10, 100, 200, 300]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline_concussion,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1_weighted',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_concussion, y_concussion)\n",
    "\n",
    "best_pipeline_concussion = grid_search.best_estimator_\n",
    "\n",
    "y_pred_concussion = cross_val_predict(best_pipeline_concussion, X_concussion, y_concussion, cv=5)\n",
    "\n",
    "accuracy_concussion = accuracy_score(y_concussion, y_pred_concussion)\n",
    "precision_concussion = precision_score(y_concussion, y_pred_concussion, average=\"weighted\") \n",
    "f1_concussion = f1_score(y_concussion, y_pred_concussion, average=\"weighted\")\n",
    "\n",
    "print(f\"Best C value: {grid_search.best_params_['model__C']}\")\n",
    "print(f\"Cross-validated Accuracy for Concussion Dataset: {accuracy_concussion:.4f}\")\n",
    "print(f\"Cross-validated Precision for Concussion Dataset: {precision_concussion:.4f}\")\n",
    "print(f\"Cross-validated F1-score for Concussion Dataset: {f1_concussion:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
